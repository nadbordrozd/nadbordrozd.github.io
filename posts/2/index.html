
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>DS lore</title>
  <meta name="author" content="nadbor">

  
  <meta name="description" content="I will share with you a snippet that took out a lot of misery from my dealing with pyspark dataframes. This is pysparks-specific. Nothing to see here &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://nadbordrozd.github.io/posts/2/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="DS lore" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65624880-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">DS lore</a></h1>
  
    <h2>words about stuff</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="nadbordrozd.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/interviews">Interviews</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/22/one-weird-trick-that-will-fix-your-pyspark-schemas/">Data Engineers Will Hate You - One Weird Trick to Fix Your Pyspark Schemas</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-05-22T21:39:15+01:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>22</span><span class='date-suffix'>nd</span>, <span class='date-year'>2016</span></span> <span class='time'>9:39 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I will share with you a snippet that took out a lot of misery from my dealing with pyspark dataframes. This is pysparks-specific. Nothing to see here if you’re not a pyspark user. The first two sections consist of me complaining about schemas and the remaining two offer what I think is a neat way of creating a schema from a dict (or a dataframe from an rdd of dicts).</p>

<h4 id="the-good-the-bad-and-the-ugly-of-dataframes">The Good, the Bad and the Ugly of dataframes</h4>
<p>Dataframes in pyspark are simultaneously pretty great and <del>kind of</del> completely broken.</p>

<ul>
  <li>they enforce a schema</li>
  <li>you can run SQL queries against them</li>
  <li>faster than rdd</li>
  <li>much smaller than rdd when stored in parquet format</li>
</ul>

<p>On the other hand:</p>

<ul>
  <li>dataframe join sometimes gives <a href="https://issues.apache.org/jira/browse/SPARK-10892">wrong results</a></li>
  <li>pyspark dataframe outer join acts as an inner join</li>
  <li>when cached with <code>df.cache()</code> dataframes sometimes start throwing <code>key not found</code> and Spark driver dies. Other times the task succeeds but the the underlying rdd becomes corrupted (field values switched up).</li>
  <li>not really dataframe’s fault but related - parquet is not human readable which sucks - can’t easily inspect your saved dataframes</li>
</ul>

<p>But the biggest problem is actually transforming the data. It works perfectly on those contrived examples from the tutorials. But I’m not working with flat SQL-table-like datasets. Or if I am, they are already in some SQL database. When I’m using Spark, I’m using it to work with messy multilayered json-like objects. If I had to create a UDF and type out a ginormous schema for every transformation I want to perform on the dataset, I’d be doing nothing else all day, I’m not even joking. UDFs in pyspark are clunky at the best of times but in my typical usecase they are unusable. Take this, relatively tiny record for instance:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">record</span> <span class="o">=</span> <span class="p">{</span>
</span><span class="line">    <span class="s">&#39;first_name&#39;</span><span class="p">:</span> <span class="s">&#39;nadbor&#39;</span><span class="p">,</span>
</span><span class="line">    <span class="s">&#39;last_name&#39;</span><span class="p">:</span> <span class="s">&#39;drozd&#39;</span><span class="p">,</span>
</span><span class="line">    <span class="s">&#39;occupation&#39;</span><span class="p">:</span> <span class="s">&#39;data scientist&#39;</span><span class="p">,</span>
</span><span class="line">    <span class="s">&#39;children&#39;</span><span class="p">:</span> <span class="p">[</span>
</span><span class="line">        <span class="p">{</span>
</span><span class="line">            <span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Lucja&#39;</span><span class="p">,</span>
</span><span class="line">            <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
</span><span class="line">            <span class="s">&#39;likes cold showers&#39;</span><span class="p">:</span> <span class="bp">True</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">    <span class="p">]</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>the correct schema for this is created like this:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">BooleanType</span><span class="p">,</span> <span class="n">ArrayType</span><span class="p">,</span> <span class="n">IntegerType</span>
</span><span class="line"><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span>
</span><span class="line">        <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;first_name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
</span><span class="line">        <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;last_name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
</span><span class="line">        <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;occupation&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
</span><span class="line">        <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;children&quot;</span><span class="p">,</span> <span class="n">ArrayType</span><span class="p">(</span>
</span><span class="line">            <span class="n">StructType</span><span class="p">([</span>
</span><span class="line">                <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;name&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
</span><span class="line">                <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;age&quot;</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">),</span>
</span><span class="line">                <span class="n">StructField</span><span class="p">(</span><span class="s">&quot;likes cold schowers&quot;</span><span class="p">,</span> <span class="n">BooleanType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)</span>
</span><span class="line">            <span class="p">])</span>
</span><span class="line">        <span class="p">),</span> <span class="bp">True</span><span class="p">)</span>
</span><span class="line">    <span class="p">])</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>And this is what I would have to type every time I need a udf to return such record - which can be many times in a single spark job.</p>

<h4 id="dataframe-from-an-rdd---how-it-is">Dataframe from an rdd - how it is</h4>
<p>For these reasons (+ legacy json job outputs from hadoop days) I find myself switching back and forth between dataframes and rdds. Read some JSON dataset into an rdd, transform it, join with another, transform some more, convert into a dataframe and save as parquet. Or read some parquet files into a dataframe, convert to rdd, do stuff to it, convert back to dataframe and save as parquet again. This workflow is not so bad - I get the best of both worlds by using rdds and dataframes only for the things they’re good at. How do you go from a dataframe to an rdd of dictionaries? This part is easy:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">rdd</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">asDict</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>It’s the other direction that is problematic. You would think that rdd’s method <code>toDF()</code> would do the job but no, it’s broken.</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">df</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>actually returns a dataframe with the following schema (<code>df.printSchema()</code>):</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="text"><span class="line">root
</span><span class="line"> |-- children: array (nullable = true)
</span><span class="line"> |    |-- element: map (containsNull = true)
</span><span class="line"> |    |    |-- key: string
</span><span class="line"> |    |    |-- value: boolean (valueContainsNull = true)
</span><span class="line"> |-- first_name: string (nullable = true)
</span><span class="line"> |-- last_name: string (nullable = true)
</span><span class="line"> |-- occupation: string (nullable = true)
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It interpreted the inner dictionary as a <code>map</code> of <code>boolean</code> instead of a <code>struct</code> and <em>silently dropped</em> all the fields in it that are not booleans. But this method is deprecated now anyway. The preferred, official way of creating a dataframe is with an rdd of <code>Row</code> objects. So let’s do that.</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
</span><span class="line"><span class="n">rdd_of_rows</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="o">**</span><span class="n">x</span><span class="p">))</span>
</span><span class="line"><span class="n">df</span> <span class="o">=</span> <span class="n">sql</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd_of_rows</span><span class="p">)</span>
</span><span class="line"><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>prints the same schema as the previous method.</p>

<p>In addition to this, both these methods will fail completely when some field’s type cannot be determined because all the values happen to be null in some run of the job.</p>

<p>Also, quite bizarrely in my opinion, order of columns in a dataframe is significant while the order of keys is not. So if you have a pre-existing schema and you try contort an rdd of dicts into that schema, you’re gonna have a bad time.</p>

<h4 id="how-it-should-be">How it should be</h4>
<p>Without further ado, this is how I now create my dataframes:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="c"># this is what a typical record in the rdd looks like</span>
</span><span class="line"><span class="n">prototype</span> <span class="o">=</span> <span class="p">{</span>
</span><span class="line">    <span class="s">&#39;first_name&#39;</span><span class="p">:</span> <span class="s">&#39;nadbor&#39;</span><span class="p">,</span>
</span><span class="line">    <span class="s">&#39;last_name&#39;</span><span class="p">:</span> <span class="s">&#39;drozd&#39;</span><span class="p">,</span>
</span><span class="line">    <span class="s">&#39;occupation&#39;</span><span class="p">:</span> <span class="s">&#39;data scientist&#39;</span><span class="p">,</span>
</span><span class="line">    <span class="s">&#39;children&#39;</span><span class="p">:</span> <span class="p">[</span>
</span><span class="line">        <span class="p">{</span>
</span><span class="line">            <span class="s">&#39;name&#39;</span><span class="p">:</span> <span class="s">&#39;Lucja&#39;</span><span class="p">,</span>
</span><span class="line">            <span class="s">&#39;age&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
</span><span class="line">            <span class="s">&#39;likes cold showers&#39;</span><span class="p">:</span> <span class="bp">True</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">    <span class="p">]</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line"><span class="n">df</span> <span class="o">=</span> <span class="n">df_from_rdd</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">prototype</span><span class="p">,</span> <span class="n">sqlContext</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This doesn’t randomly break, doesn’t drop fields and has the right schema. And I didn’t have to type any of this <code>StructType([StructField(...</code> nonsense, just plain python literal that I got by running</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">print</span> <span class="n">rdd</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>As an added bonus now this prototype is prominently displayed at the top of my job file and I can tell what the output of the job looks like without having to decode parquet files. Self documenting code FTW!</p>

<h4 id="how-to-get-there">How to get there</h4>
<p>And here’s how it’s done. First we need to implement our own schema inference - the way it should work:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">pyspark.sql.types</span> <span class="kn">as</span> <span class="nn">pst</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">infer_schema</span><span class="p">(</span><span class="n">rec</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;infers dataframe schema for a record. Assumes every dict is a Struct, not a Map&quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rec</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">pst</span><span class="o">.</span><span class="n">StructType</span><span class="p">([</span><span class="n">pst</span><span class="o">.</span><span class="n">StructField</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">infer_schema</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="bp">True</span><span class="p">)</span>
</span><span class="line">                              <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">rec</span><span class="o">.</span><span class="n">items</span><span class="p">())])</span>
</span><span class="line">    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rec</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rec</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;can&#39;t infer type of an empty list&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="n">elem_type</span> <span class="o">=</span> <span class="n">infer_schema</span><span class="p">(</span><span class="n">rec</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">rec</span><span class="p">:</span>
</span><span class="line">            <span class="n">this_type</span> <span class="o">=</span> <span class="n">infer_schema</span><span class="p">(</span><span class="n">elem</span><span class="p">)</span>
</span><span class="line">            <span class="k">if</span> <span class="n">elem_type</span> <span class="o">!=</span> <span class="n">this_type</span><span class="p">:</span>
</span><span class="line">                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;can&#39;t infer type of a list with inconsistent elem types&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">pst</span><span class="o">.</span><span class="n">ArrayType</span><span class="p">(</span><span class="n">elem_type</span><span class="p">)</span>
</span><span class="line">    <span class="k">else</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="n">pst</span><span class="o">.</span><span class="n">_infer_type</span><span class="p">(</span><span class="n">rec</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Using this we can now specify the schema using a regular python object - no more java-esque abominations. But this is not all. We will also need a function that transforms a python dict into a rRw object with the correct schema. You would think that this should be automatic as long as the dict has all the right fields, but no - order of fields in a Row is significant, so we have to do it ourselves.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">_rowify</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototype</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;creates a Row object conforming to a schema as specified by a dict&quot;&quot;&quot;</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">_equivalent_types</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">unicode</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">unicode</span><span class="p">]:</span>
</span><span class="line">            <span class="k">return</span> <span class="bp">True</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">None</span>
</span><span class="line">    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prototype</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">dict</span><span class="p">:</span>
</span><span class="line">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;expected dict, got </span><span class="si">%s</span><span class="s"> instead&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span class="line">        <span class="n">rowified_dict</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line">        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span><span class="line">            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">prototype</span><span class="p">:</span>
</span><span class="line">                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;got unexpected field </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">key</span><span class="p">)</span>
</span><span class="line">            <span class="n">rowified_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">_rowify</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">prototype</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
</span><span class="line">            <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">prototype</span><span class="p">:</span>
</span><span class="line">                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
</span><span class="line">                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
</span><span class="line">                        <span class="s">&quot;expected </span><span class="si">%s</span><span class="s"> field but didn&#39;t find it&quot;</span> <span class="o">%</span> <span class="n">key</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">Row</span><span class="p">(</span><span class="o">**</span><span class="n">rowified_dict</span><span class="p">)</span>
</span><span class="line">    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prototype</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">:</span>
</span><span class="line">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;expected list, got </span><span class="si">%s</span><span class="s"> instead&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span><span class="line">        <span class="k">return</span> <span class="p">[</span><span class="n">_rowify</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">prototype</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
</span><span class="line">    <span class="k">else</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="ow">not</span> <span class="n">_equivalent_types</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototype</span><span class="p">):</span>
</span><span class="line">            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&quot;expected </span><span class="si">%s</span><span class="s">, got </span><span class="si">%s</span><span class="s"> instead&quot;</span> <span class="o">%</span>
</span><span class="line">                             <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">prototype</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span><span class="line">        <span class="k">return</span> <span class="n">x</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>And finally:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">df_from_rdd</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">prototype</span><span class="p">,</span> <span class="n">sql</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;creates a dataframe out of an rdd of dicts, with schema inferred from a prototype record&quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">schema</span> <span class="o">=</span> <span class="n">infer_schema</span><span class="p">(</span><span class="n">prototype</span><span class="p">)</span>
</span><span class="line">    <span class="n">row_rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">_rowify</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prototype</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">sql</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">row_rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2016/05/20/text-classification-with-word2vec/">Text Classification With Word2Vec</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-05-20T18:18:58+01:00'><span class='date'><span class='date-month'>May</span> <span class='date-day'>20</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>6:18 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In the <a href="http://nadbordrozd.github.io/blog/2015/11/29/ds-toolbox-topic-models/">previous post</a> I talked about usefulness of topic models for non-NLP tasks, it’s back to NLP-land this time. I decided to investigate if word embeddings can help in a classic NLP problem - text categorization. Full code used to generate numbers and plots in this post can be found <a href="https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/benchmarking.ipynb">here</a>.</p>

<h4 id="motivation">Motivation</h4>
<p>The basic idea is that semantic vectors (such as the ones provided by Word2Vec) should preserve most of the relevant information about a text while having relatively low dimensionality which allows better machine learning treatment than straight one-hot encoding of words. Another advantage of topic models is that they are unsupervised so they can help when labaled data is scarce. Say you only have one thousand manually classified blog posts but a million unlabeled ones. A high quality topic model can be trained on the full set of one million. If you can use topic modeling-derived features in your classification, you will be benefitting from your entire collection of texts, not just the labeled ones.</p>

<h4 id="getting-the-embedding">Getting the embedding</h4>
<p>Ok, word embeddings are awesome, how do we use them? Before we do anything we need to get the vectors. We can download one of the great pre-trained models from <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a>:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">wget http://nlp.stanford.edu/data/glove.6B.zip
</span><span class="line">unzip glove.6B.zip
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>and use load them up in python:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;glove.6B.50d.txt&quot;</span><span class="p">,</span> <span class="s">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">lines</span><span class="p">:</span>
</span><span class="line">    <span class="n">w2v</span> <span class="o">=</span> <span class="p">{</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]))</span>
</span><span class="line">           <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>or we can train a Word2Vec model from scratch with gensim:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">gensim</span>
</span><span class="line"><span class="c"># let X be a list of tokenized texts (i.e. list of lists of tokens)</span>
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</span><span class="line"><span class="n">w2v</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2word</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">syn0</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="the-python-meat">The (python) meat</h4>
<p>We got ourselves a dictionary mapping word -&gt; 100-dimensional vector. Now we can use it to build features. The simplest way to do that is by averaging word vectors for all words in a text. We will build a sklearn-compatible transformer that is initialised with a word -&gt; vector dictionary.</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">class</span> <span class="nc">MeanEmbeddingVectorizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word2vec</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span> <span class="o">=</span> <span class="n">word2vec</span>
</span><span class="line">        <span class="c"># if a text is empty we should return a vector of zeros</span>
</span><span class="line">        <span class="c"># with the same dimensionality as all the other vectors</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">itervalues</span><span class="p">()</span><span class="o">.</span><span class="n">next</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span><span class="line">            <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span><span class="p">]</span>
</span><span class="line">                    <span class="ow">or</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">            <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">X</span>
</span><span class="line">        <span class="p">])</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Let’s throw in a version that uses tf-idf weighting scheme for good measure</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">class</span> <span class="nc">TfidfEmbeddingVectorizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word2vec</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span> <span class="o">=</span> <span class="n">word2vec</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">word2weight</span> <span class="o">=</span> <span class="bp">None</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">itervalues</span><span class="p">()</span><span class="o">.</span><span class="n">next</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">        <span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
</span><span class="line">        <span class="n">tfidf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span><span class="line">        <span class="c"># if a word was never seen - it must be at least as infrequent</span>
</span><span class="line">        <span class="c"># as any of the known words - so the default idf is the max of </span>
</span><span class="line">        <span class="c"># known idf&#39;s</span>
</span><span class="line">        <span class="n">max_idf</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">tfidf</span><span class="o">.</span><span class="n">idf_</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">word2weight</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
</span><span class="line">            <span class="k">lambda</span><span class="p">:</span> <span class="n">max_idf</span><span class="p">,</span>
</span><span class="line">            <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">idf_</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span><span class="line">                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2weight</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
</span><span class="line">                         <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span><span class="p">]</span> <span class="ow">or</span>
</span><span class="line">                        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">                <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">X</span>
</span><span class="line">            <span class="p">])</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>These vectorizers can now be used <em>almost</em> the same way as <code>CountVectorizer</code> or <code>TfidfVectorizer</code> from <code>sklearn.feature_extraction.text</code>. Almost - because sklearn vectorizers can also do their own tokenization - a feature which we won’t be using anyway because the benchmarks we will be using come already tokenized. In a real application I wouldn’t trust sklearn with tokenization anyway - rather let spaCy do it.</p>

<p>Now we are ready to define the actual models that will take tokenised text, vectorize and learn to classify the vectors with something fancy like Extra Trees. sklearn’s Pipeline is perfect for this:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
</span><span class="line">
</span><span class="line"><span class="n">etree_w2v</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
</span><span class="line">    <span class="p">(</span><span class="s">&quot;word2vec vectorizer&quot;</span><span class="p">,</span> <span class="n">MeanEmbeddingVectorizer</span><span class="p">(</span><span class="n">w2v</span><span class="p">)),</span>
</span><span class="line">    <span class="p">(</span><span class="s">&quot;extra trees&quot;</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">))])</span>
</span><span class="line"><span class="n">etree_w2v_tfidf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
</span><span class="line">    <span class="p">(</span><span class="s">&quot;word2vec vectorizer&quot;</span><span class="p">,</span> <span class="n">TfidfEmbeddingVectorizer</span><span class="p">(</span><span class="n">w2v</span><span class="p">)),</span>
</span><span class="line">    <span class="p">(</span><span class="s">&quot;extra trees&quot;</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">))])</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="benchmarks">Benchmarks</h4>
<p>I benchmarked the models on everyone’s favorite <a href="http://www.cs.umb.edu/~smimarog/textmining/datasets/">Reuters-21578</a> datasets. Extra Trees-based word-embedding-utilising models competed against text classification classics - Naive Bayes and SVM. Full list of contestants:</p>

<ul>
  <li>mult_nb - Multinomial Naive Bayes</li>
  <li>bern_nb - Bernoulli Naive Bayes</li>
  <li>svc - linear kernel SVM</li>
  <li>glove_small - ExtraTrees with 200 trees and vectorizer based on 50-dimensional gloVe embedding trained on 6B tokens</li>
  <li>glove_big - same as above but using 300-dimensional gloVe embedding trained on 840B tokens</li>
  <li>w2v - same but with using 100-dimensional word2vec embedding trained on the benchmark data itself (using both training and test examples [but not labels!])</li>
</ul>

<p>Each of these came in two varieties - regular and tf-idf weighted.</p>

<p>The results (on 5-fold cv on a the R8 dataset of 7674 texts labeled with 8 categories):</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="bash"><span class="line">model                score
</span><span class="line">-----------------  -------
</span><span class="line">svc_tfidf           0.9656
</span><span class="line">svc                 0.9562
</span><span class="line">w2v_tfidf           0.9544
</span><span class="line">w2v                 0.9510
</span><span class="line">mult_nb             0.9467
</span><span class="line">glove_big           0.9274
</span><span class="line">glove_small         0.9262
</span><span class="line">glove_small_tfidf   0.9075
</span><span class="line">glove_big_tfidf     0.9038
</span><span class="line">mult_nb_tfidf       0.8615
</span><span class="line">bern_nb             0.7954
</span><span class="line">bern_nb_tfidf       0.7954
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>SVM wins, word2vec-based Extra Trees is a close second, Naive Bayes not far behind. Interestingly, embedding trained on this relatively tiny dataset does significantly better than pretrained GloVe - which is otherwise fantastic. Can we do better? Let’s check how do the models compare depending on the number of labeled training examples. Due to its semi-supervised nature w2v should shine when there is little labeled data.</p>

<p><img src="/images/text_class_with_w2v/r8.png" /></p>

<p>That indeed seems to be the case. <code>w2v_tfidf</code>’s performance degrades most gracefully of the bunch. <code>SVM</code> takes the biggest hit when examples are few. Lets try the other two benchmarks from Reuters-21578. 52-way classification:</p>

<p><img src="/images/text_class_with_w2v/r52.png" /></p>

<p>Qualitatively similar results.</p>

<p>And 20-way classification:</p>

<p><img src="/images/text_class_with_w2v/20ng.png" /></p>

<p>This time pretrained embeddings do better than Word2Vec and Naive Bayes does really well, otherwise same as before.</p>

<h4 id="conclusions">Conclusions</h4>
<ol>
  <li>SVM’s are pretty great at text classification tasks</li>
  <li>Models based on simple averaging of word-vectors can be surprisingly good too (given how much information is lost in taking the average)</li>
  <li>but they only seem to have a clear advantage when there is ridiculously little labeled training data</li>
</ol>

<p>At this point I have to note that averaging vectors is only the easiest way of leveraging word embeddings in classification but not the only one. You could also try embedding whole documents directly with <a href="https://radimrehurek.com/gensim/models/doc2vec.html">Doc2Vec</a>. Or use Multinomial Gaussian Naive Bayes on word vectors. I have tried the <a href="https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/multi_multi_kernel_nb.py">latter approach</a> but it was too slow to include in the benchmark.</p>

<p><em>Update 2017: actually, the best way to utilise the pretrained embeddings would probably be this https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html I shall add this approach to the benchmark when I have some time.</em></p>

<ol>
  <li>Sometimes pretrained embeddings give clearly superior results to word2vec trained on the specific benchmark, sometimes it’s the opposite. Not sure what is going on here.</li>
</ol>

<p>Overall, we won’t be throwing away our SVMs any time soon in favor of word2vec but it has it’s place in text classification. Like when you have a tiny training set or to ensemble it with other models to gain edge in Kaggle.</p>

<p>Plus, can SVM do this:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="s">&#39;Berlin&#39;</span><span class="p">,</span> <span class="s">&#39;London&#39;</span><span class="p">],</span>
</span><span class="line">     <span class="p">[</span><span class="s">&#39;cow&#39;</span><span class="p">,</span> <span class="s">&#39;cat&#39;</span><span class="p">],</span>
</span><span class="line">     <span class="p">[</span><span class="s">&#39;pink&#39;</span><span class="p">,</span> <span class="s">&#39;yellow&#39;</span><span class="p">]]</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;capitals&#39;</span><span class="p">,</span> <span class="s">&#39;animals&#39;</span><span class="p">,</span> <span class="s">&#39;colors&#39;</span><span class="p">]</span>
</span><span class="line"><span class="n">etree_glove_big</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="c"># never before seen words!!!</span>
</span><span class="line"><span class="n">test_X</span> <span class="o">=</span> <span class="p">[[</span><span class="s">&#39;dog&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;red&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s">&#39;Madrid&#39;</span><span class="p">]]</span>
</span><span class="line">
</span><span class="line"><span class="k">print</span> <span class="n">etree_glove_big</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>prints</p>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class="text"><span class="line">[&#39;animals&#39; &#39;colors&#39; &#39;capitals&#39;]
</span></code></pre></td></tr></table></div></figure></notextile></div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/11/29/ds-toolbox-topic-models/">DS Toolbox - Topic Models</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-11-29T14:59:33+00:00'><span class='date'><span class='date-month'>Nov</span> <span class='date-day'>29</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>2:59 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>If you’re not primarily working with NLP you may not have been paying attention to topic modeling and word embeddings. In this post I intend to convince you that you should.</p>

<h3 id="topic-models">Topic models</h3>
<p>Topic models are a set of models in NLP that discover common themes in a collection of documents. You give it a list of texts and it comes up with a bunch of topics and maps every document to a topic or a mixture of topics. <a href="http://www.princeton.edu/~achaney/tmve/wiki100k/browse/topic-presence.html">Here</a> you can see a visualization of a topic model applied to wikipedia articles. As you can see, it picks up similar kinds of themes in the texts that a human being would notice. Take topic 5 for instance. Its top relevant words are “war”, “force”, “army”, “attack”, “military” and top articles: “Second Boer War”, “Erwin Rommel”, “Axis powers”, “Vietnam war”. Pretty neat. Most topic models (like the most popular <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA</a> produce a vector of numbers for every text - the distribution of topics and a similar vector for every word - the affinity of the word to every topic.</p>

<h3 id="word-and-document-embeddings">Word (and document) embeddings</h3>
<p>Word embeddings are related to topic models but instead of mapping a text to a mixture of topics they learn to map words to real valued fixed-size vectors. The mapping is designed to preserve the semantic structure so that the distance between vectors corresponds to distance in meaning of words. An additional property of these embeddings (probably unintended) is that you can do a sort of algebra on words - like this:</p>

<script type="math/tex; mode=display">vector("Paris") - vector("France") + vector("Italy") \approx vector("Rome")</script>

<p>- an examples taken from the most famous word embedding algorithm  -<a href="https://code.google.com/p/word2vec/">word2vec</a>. How cool is that?
 If you want to play with the vectors yourself and discover new fun analogies ($Bush - Iraq + Vietnam \approx Nixon$ !!!) you can download pretrained vectors from word2vec or <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a>.</p>

<h3 id="why-you-should-care">Why you should care</h3>
<p>Unsupervised algorithms learning about analogies between real-world entities is pretty cool but it obscures the wider applicability of these algorithms. The topic modeling and embedding algorithms were invented in the context of NLP but most of them (including LDA and wor2vec) don’t have any inherent knowledge about natural languages and can be applied just as well to sequences of items from a discrete set of any kind. This is huge. Sequences (or sets) of items are notoriously hard to work with. Most popular ML algorithms expect fixed-size vectors as inputs. If your inputs are sequences you can either:</p>

<ul>
  <li>do one-hot encoding and then use any old ML algo you want (like a random forest). Unfortunately one-hot discards all information about the order of items. More importantly, as the size of the vocabulary grows beyond thousands (which is still tiny as far as vocabularies go) your random forest will take forever to train and overfit.</li>
  <li>use naive bayes. NB works surprisingly well but only for classification and only when the problem is - well, naive. It also utilizes the item order information to a very limited extent via n-grams.</li>
  <li>use a <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">recurrent neural network</a> This can actually be very effective in some cases but I don’t think it would work well with a vocabulary size in the thousands. Even the character-level language models take days to train properly on a GPU (but what incredible results it produces!). I believe NNs of all kinds will get better and easier to use but as of now this is not a practical solution to our problem at all.</li>
</ul>

<p>This is where word embeddings come in. Just run word2vec on your sequences of items and you’ll get a reasonably-dimensional representation of every item. You can then take the mean of all vectors in a sequence to get a representation of sequences. Or run <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">doc2vec</a> and you’ll get a vector for each sequence directly. Or if you need them clustered - run LDA. LDA’s word-topic coefficients can also be used as word embedding but they don’t do nearly as good of a job at it as word2vec. Same goes for LDA’s text-topic coefficients as a document to vector mapping.</p>

<p>This is it. This is what topic modeling buys you. It’s a generic clustering and feature extraction technique that works on sequences (or sets) of items from a discrete vocabulary. Does it actually work in practice? I don’t know of a lot of examples of people using it but I know a few.</p>

<h4 id="page-jumps">Page jumps</h4>
<p>As I have already described in my <a href="http://nadbordrozd.github.io/interviews/">mini-guide to data science interviews</a> (question “Predicting page jumps”) it can be used to model users jumping between pages of a web application. Here a page plays the role of a word and a user journey is a sentence. You can (and from what I understood from the interview - they [the company I interviewed with] do) use topic modeling to segment users based on their journeys and extract features for them to predict page jumps.</p>

<h4 id="ad-clicks">Ad clicks</h4>
<p>In a similar vein, topic modeling <a href="http://www.cs.kumamoto-u.ac.jp/~yasuko/PUBLICATIONS/kdd12-trimine.pdf">has been used</a> as a feature extraction technique in the prediction of ad clicks. The paper concentrates on the authors’ special brand of topic modeling but the idea is simple and can be used with any topic model for example LDA. They treat hosts (the websites with banners on them) as words and users who visit the websites as documents. Let’s say John visits youtube then google then wikipedia then youtube again then tmz then guardian. This gives us a sequence [“youtube”, “google”, “wikipedia”, “youtube”, “tmz”, “guardian”]. Topic modeling is applied to the set of all users and the result is a set of topics (“media”, “business” and “drive” in the paper) and a decomposition of every user into those topics. So for our John we should expect something like <code>{media: 0.8, business: 0.2, drive: 0}</code>. This is interesting in itself and constitutes a great feature vector that you can feed into a regression predicting clicks - which is exactly what the authors did.</p>

<h4 id="deepwalk">DeepWalk</h4>
<p>Feature extraction from graphs is even harder than for sequences. Say you want to classify youtube videos. You have a labeled training set and a set of features for every video. But you also know that some videos link to other videos - forming a graph. Information about the position of the video in this graph is bound to be useful for classifiaction (think “oh, I’m in that weird part of youtube again”) but how do you use it. Authors of the <a href="http://arxiv.org/abs/1403.6652">DeepWalk</a> paper compared several approaches and the best turned out to be a trick involving word2vec that they invented. This time the role of a word is played by a node in the graph (a youtube video). What plays the role of a sentence? For that a collection of short random walks on the graph is constructed - starting at every node of the graph. Think about what happens when a youtube video ends and another one starts playing, and then another one and another. Do this a few times for every video on youtube and you have a corpus of texts. Authors of the paper applied word2vec to those sequences to get vector embeddings for videos and then used these vectors as features in classification. It worked better then all other approaches - even ones that use global features of the graph. Awesome.</p>

<p><em>Update July 2016</em> <br />
I have tried this very algorithm on the graph of UK and Irish companies, <a href="http://nadbordrozd.github.io/blog/2016/06/13/deepwalking-with-companies/">here are the results</a></p>

<h4 id="frequent-itemsets--collaborative-filtering">Frequent itemsets / collaborative filtering</h4>
<p>Association rule learning is a popular task in the context of retail. If a customer bought butter and bread, what other item are they likely to buy? The usual approach is to count instances of people buying {bread, butter, X} and divide that number by the count of people buying {bread, butter} - this estimates the probability of buying X. Then you can find the X that maximizes the probability and do something with it (suggest it to the user as they are about to checkout perhaps). This is a bit crude, not very robust and it doesn’t provide any insight, just the prediction. What you can do instead is to run (you guessed it) topic modeling with items playing the role of words and baskets playing the role of sentences. Word2vec will give you vector representations of both items and baskets which will allow you to use more sophisticated algorithms for predicting the next item. You will also get a segmentation of all users and all items for free. To understand why this is superior consider this: topic modeling will easily pick up on the cluster of vegetarian buyers and then the model will know not to recommend the buyer pork chops even if they bought three other items that usually go with bacon - this is something frequent itemset algorithms are incapable of. When a new type of soy-based pork substitute appears on the shelves, the algorithm will also take much less time to figure out that it belongs to the vegetarian cluster and is analogous to meat. I don’t actually know if anyone in retail is doing topic modeling on baskets but if they don’t, they should. I’ll do it myself if I can find a free dataset with retail baskets.</p>

<p><em>Update July 2016</em> <br />
Called it. People are totally doing that now <a href="http://arxiv.org/abs/1603.04259">here</a> and <a href="https://arxiv.org/abs/1601.01356">here</a>. I don’t know why in the above text I fixated on frequent itemsets - which is just a specific, outdated way of doing collaborative filtering.</p>

<p>If you know of any other cool applications of topic modeling to non-NLP problems let me know.</p>

<p>If you want to play with topic models yourself I wholehartedly recommend <a href="https://radimrehurek.com/gensim/">gensim</a>. I tried also MLLib but its word2vec implementation required 3 times as much RAM (for each of 10 cores I used) and still was about ten times slower than gensim.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/08/01/forgetting-salesman-problem-or-classification-without-negative-examples/">Lead Scoring Without Negative Examples</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-08-01T20:49:26+01:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>1</span><span class='date-suffix'>st</span>, <span class='date-year'>2015</span></span> <span class='time'>8:49 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>How do you train a binary classifier when you have only positive-labeled training examples? 
Impossible? Maybe. But sometimes you can do something just as good. Let’s start from the beginning…</p>

<h3 id="lead-generation">Lead generation</h3>
<p>Everyone and their mum in the b2b sector is trying to use machine learning 
for lead generation. For our purposes lead-gen means selecting from the millions of active companies in your country the ones that are most likely to buy your product once contacted by your sales team. Needless to say lead generation is extremely <em>serious business</em> and constitutes a market sector in its own right - a type of b2b2b if you will.</p>

<p>One popular lead-gen strategy is to look for leads among companies similar the ones that have worked out for you in the past. A machine learning implementation of this strategy is straightforward:</p>

<ol>
  <li>collect data on all relevant companies</li>
  <li>take all the companies your sales have contacted in the past and label every one as either successful (you made money) or failed (you wasted your time) lead. This is your training set.</li>
  <li>extract good features for every company*</li>
  <li>train a binary classifier (predicting successful/failed label) on your training set. Ideally this should be a classifier that outputs probabilities.</li>
  <li>apply the classifier to all the companies outside the training set and hand the results over to sales.
The number assigned by the classifier to a given company is (in theory at least) the probability that the lead will be successful. Highest scorers should be prioritized, low scorers should be left alone. So far, so good.</li>
</ol>

<h3 id="forgetting-salesman-problem">Forgetting salesman problem</h3>
<p>An interesting difficulty frequently arises when you try to apply this approach in real life. Sales teams usually have good records on their clients but often fail to keep track of all the prospects that didn’t work out. As a result, you end up with only positive examples to train your classifier on. It’s not possible in general to train a classifier on a training set with one class amputated this way. But…</p>

<h3 id="the-solution">The solution</h3>
<p>TL;DR = divide density of “good points” by density of all - this is the lead score</p>

<p>Keep in mind that:</p>

<ul>
  <li>in addition to the positive examples, you have also the full set of all data points (all companies whether previously contacted or not)</li>
  <li>you are not interested in the absolute probability of success of a given lead per se. You only really need the relative ranking of different leads. All the classifier scores may be off by a factor of 10 but as long as the order is preserved - you’ll still be able to prioritize good leads over rubbish ones.</li>
</ul>

<p>A solution will present itself once the problem is stated in mathematical terms. Consider the space of all possible company features $X$. Given are:</p>

<ul>
  <li>a set $C$ of points in $X$ - all companies</li>
  <li>
    <p>a subset $T_+$ of the above set - companies that were tested and gave positive result
Unknown are:</p>
  </li>
  <li>the probability $f(x)$ that a company $x$ will test positive</li>
  <li>the subset $T$ of all tested companies</li>
</ul>

<p>This is how it looks in an artificial 1-D dataset.
<img src="/images/leadgen1.png" /></p>

<p>Notice that the distribution of companies $C$ and the conversion probability $f$ peak at different points and the distribution of positive exmaples $T_+$ peaks somewhere between those two.</p>

<p>It’s useful to think of $C$ as a sample from some probabilty distribution $P(x)$ defined on $X$. Or to express the same in the language of point densities in space:</p>

<script type="math/tex; mode=display">density\_of\_C(x) = const_1 P(x)</script>

<p>If the companies tested $T$ were chosen at random from all companies $C$ then they constitute a sample from the same probability distribution $P(x)$</p>

<script type="math/tex; mode=display">density\_of\_T(x) = const_2 P(x)</script>

<p>Since a company’s chance of testing positive is $f$, the density of companies that tested positive is simply:</p>

<script type="math/tex; mode=display">density\_of\_T_+(x) = density\_of\_T(x) f(x) = const_2 P(x) f(x)</script>

<p>The set $T$ is hidden from us but $C$ and $T_+$ are available so we can use them to find $f(x)$. Combining the first and third equation we get:</p>

<script type="math/tex; mode=display">\frac{const_2}{const_1} f(x) = \frac{density\_of\_T_+(x)}{density\_of\_C(x)}</script>

<p>This is it. If we can just estimate the densities of $C$ and $T_+$ - we’re golden. Their ratio gives the $f$ function up to a constant, which is all we need to do lead scoring. There are many ways to do density estimation in practice:</p>

<ul>
  <li>histogram based estimation - lowest on the sophistication scale - divide the space $X$ into sectors and count the number of points in every sector, divide it by volume. Do it for both $C$ and $T_+$ and divide one result by the other</li>
  <li>nearest neighbour estimation - for any interesting point find $k$ (let’s say $k=20$) nearest points in $C$. Count how many of them are also in $T_+$, divide the figures</li>
  <li>kernel based estimation - in a way this is similar to the knn version - except instead of taking k nearest points you take all of them but discount the farthest by a dicreasing function of distance</li>
</ul>

<p>This is how it looks on our toy dataset:
<img src="/images/leadgen2.png" /></p>

<p>Unsuprisingly, on a toy model everything works rather well. In reality conversion rates are low, data is sparse, dimensionality is high and even the basic assumption “good lead means similar to what has worked in the past” is questionable. But you have to start somewhere.</p>

<p>For more information about the ML aspect of classification without negatives google the term “PU learning”.</p>

<p>* this is arguably the hardest and most important step, but I’m glossing over it as it’s not relevant to this 
  post</p>

<p>** if there is prior knowledge regarding how the contacted companies were chosen, it can be easily incorporated</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/26/test-post/">Test Post</a></h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2015-07-26T22:20:42+01:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>26</span><span class='date-suffix'>th</span>, <span class='date-year'>2015</span></span> <span class='time'>10:20 pm</span></time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This is my first post. pls work pls
This is code span (hopefully)
<code>def fibo(n):
	if n &lt; 2:
		return 1
	else:
		return fibo(n - 1) + fibo(n - 2)
</code></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/index.html">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/06/20/you-wont-believe-how-this-islington-single-mom-is-making-ps500-slash-day/">You Won't Believe How This Islington Single Mom Is Making £500/day</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/06/03/python-or-scala/">Python or Scala - Let the Neural Network Decide.</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/03/23/missing-data-imputation-with-pymc-part-2/">Missing Data Imputation With Pymc: Part 2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/03/05/missing-data-imputation-with-bayesian-networks/">Missing Data Imputation With Bayesian Networks in Pymc</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/09/17/text-generation-with-keras-char-rnns/">Text Generation With Keras char-RNNs</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/nadbordrozd">@nadbordrozd</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'nadbordrozd',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/nadbordrozd?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - nadbor -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ds-lore';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</html>
