
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Missing Data Imputation With Pymc: Part 2 - DS lore</title>
  <meta name="author" content="nadbor">

  
  <meta name="description" content="Missing Data Imputation With Pymc: Part 2 Mar 23rd, 2017 9:52 pm In the last post I presented a way to do Bayesian networks with pymc and use them &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://nadbordrozd.github.io/blog/2017/03/23/missing-data-imputation-with-pymc-part-2/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="DS lore" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65624880-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">DS lore</a></h1>
  
    <h2>words about stuff</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="nadbordrozd.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/interviews">Interviews</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Missing Data Imputation With Pymc: Part 2</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-03-23T21:52:41+00:00'><span class='date'><span class='date-month'>Mar</span> <span class='date-day'>23</span><span class='date-suffix'>rd</span>, <span class='date-year'>2017</span></span> <span class='time'>9:52 pm</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><p>In the last post I presented a way to do Bayesian networks with pymc and use them to impute missing data. This time I benchmark the accuracy of this method on some artificial datasets.</p>

<h3 id="datasets">Datasets</h3>
<p>In the previous posts I showed the imputation of boolean missing data, but the same method works for categorical features of any cardinality as well as continues ones (except in the continues case additional prior knowledge is required to specify the likelihood). Nevertheless, I decided to test the imputers on purely boolean datasets because it makes the scores easy to interpret and the models quick to train.</p>

<p>To make it really easy on the Bayesian imputer, I created a few artificial datasets by the following process:</p>

<ol>
  <li>define a Bayesian network</li>
  <li>sample variables corresponding to conditional probabilities describing the network from their priors (once)</li>
  <li>fix the parameters from step 2. and sample the other variables (the observed ones) repeatedly</li>
</ol>

<p>With data generated by the same Bayesian network that we will fit to it, we’re making it as easy on pymc as possible to get a good score. Mathematically speaking, the bayesian model is <em>the</em> way to do it. Anything less than optimal performance can only be due to a bug or pymc underfitting (perhaps from too few iterations).</p>

<p>The first dataset used is based on the famous <em>wet sidewalk - rain - sprinkler</em> network as seen in the <a href="https://en.wikipedia.org/wiki/Bayesian_network">wikipedia article</a> on Bayesian networks.</p>

<p><img src="/images/bayesian_networks/wet_sprinkler_rain.png" /></p>

<p>The second, bigger, is based on the <a href="http://www.causality.inf.ethz.ch/data/LUCAS.html">LUCAS</a> network</p>

<p><img src="/images/bayesian_networks/cancer_network.png" /></p>

<p>And the biggest one is based on an example <a href="http://www.igi.tugraz.at/lehre/MLB/WS10/MLB_Exercises_2010/node15.html">from some ML lecture notes</a></p>

<p><img src="/images/bayesian_networks/car_insurance.png" /></p>

<p>For each of these networks I would generate a dataframe with 10, 50, 250, 1250 or 6250 records and drop (replace with <code>-1</code>) a random subset of 20% or 50% of values in each column. Then I would try to fill them in with each model and score the model on accuracy. This was repeated 5 times for each network and data size and the accuracy reported is the mean of the 5 tries.</p>

<h3 id="models">Models</h3>
<p>The following models were used to impute the missing records:</p>

<ul>
  <li><code>most frequent</code> - a dummy model that predicts most frequent value per dataframe column. This is the absolute baseline of imputer performance, every model should be at least as good as this.</li>
  <li><code>xgboost</code> - a more ambitious machine learning-based baseline. This imputer simply trains an XGBoost Classifier for every column of the dataframe. The classifier is trained only on the records where the value of that column is not missing and it uses all the remaining columns to predict that one. So, if there are n columns - n classifiers are trained, each using n - 1 remaining columns as features.</li>
  <li><code>MAP fmin_powell</code> - a model constructed the same way as the <code>DuckImputer</code> model from the previous post. Actually, it’s a different model for each dataset, but the principle is the same. You take the very same Bayesian network that was used to create the dataset and fit it to the dataset. Then you predict the missing values using MAP with ‘method’ parameter set to ‘fmin_powell’.</li>
  <li><code>MAP fmin</code> - same as above, only with ‘method’ set to ‘fmin’. This one actually performed so poorly, (no better than random and worse than <code>most frequent</code>) and was so slow that I quickly dropped it from the benchmark</li>
  <li><code>MCMC 500</code>, <code>MCMC 2000</code>, <code>MCMC 10000</code> - same as the <code>MAP</code> models, except for the last step. Instead of finding <em>maximum a posteriori</em> for each variable directly using the MAP function, the variable is sampled n times from the posterior using MCMC, and the empirically most common value is used as the prediction. Three versions of this model were used - with 500, 2000 and 10000 iterations for burn-in repectively. After burn-in, 200 samples were used each time.</li>
</ul>

<h3 id="results">Results</h3>
<p>Let’s start with the simplest network:</p>

<p><img src="/images/bayesian_networks/rain_sprinkler_wet_accuracy20.png" alt="" />
<em>Rain-Sprinkler-Wet Sidewalk benchmark (20% missing values). Mean imputation accuracy from 5 runs vs data size.</em></p>

<p><img src="/images/bayesian_networks/rain_sprinkler_wet_time20.png" alt="" />
<em>Average fitting time in seconds. Beware log scale!</em></p>

<p>XGBoost comes out on top, bayesian models do poorly, apparently worse than even <code>most frequent</code> imputer. But variance in scores is quite big and there is not much structure in this dataset anyway, so let’s not lose hope. <code>MAP fmin_powell</code> is particularly bad and terribly slow on top of that, dropping it from further benchmarks.</p>

<p>Let’s try a wider dataset - the cancer network. This one has more structure - that the bayesian network knows up front and xgboost doesn’t - which should give bayesian models an edge.</p>

<p><img src="/images/bayesian_networks/cancer20_accuracy.png" alt="" />
<em>Cancer network imputation accuracy. 20% missing values</em></p>

<p><img src="/images/bayesian_networks/cancer20_time.png" alt="" />
<em>Cancer network imputation time.</em></p>

<p>That’s more like it! MCMC wins when records are few, but deteriorates when data gets bigger. MCMC models continue to be horribly slow.</p>

<p>And finally, the biggest (27 features!), car insurance network.</p>

<p><img src="/images/bayesian_networks/car_insurance_accuracy.png" alt="" />
<em>Car insurance network imputation accuracy. 20% missing values</em></p>

<p><img src="/images/bayesian_networks/car_insurance_time.png" alt="" />
<em>Car insurance network imputation time.</em></p>

<p>Qualitatively same as the cancer network case. It’s worth pointing out that in this case, the Bayesian models achieve at 50 records a level of accuracy that XGBoost doesn’t match until shown more than a thousand records! Still super slow though.</p>

<h3 id="conclusions">Conclusions</h3>

<p>What have we learned?</p>

<ol>
  <li>Bayesian models do relatively better when data is wide (more columns). This was expected. Wide data means bigger network, means there is more information implicit in the network structure. This is information that we hardcode into the Bayesian model, information that XGBoost doesn’t have.</li>
  <li>Bayesian models do relatively better when data is short (less records). This was also expected, for the same reason. When data is short, the information contained in the network counts for a lot. With more records to train on, this advantage gets less important.</li>
  <li>pymc’s MAP does poorly in terms of accuracy and is terribly slow, slower than MCMC. This one is a bit of a mystery.</li>
  <li>For MCMC, longer burn-in gets better results, but takes more time. Duh.</li>
  <li>MCMC model accuracy deteriorates as data gets bigger. I was surprised when I saw it, but it hindsight it’s clear that it would be the case. Bigger data means more missing values, means higher dimensionality of the space MCMC has to explore, means it would take more iterations for MCMC to  reach a high likelihood configuration. This could be alleviated if we first learned the most likely values of the parameters of the network and then used those to impute the missing values one record at a time.</li>
  <li>XGBoost rocks.</li>
</ol>

<p>Overall, I count this experiment as a successful proof of concept, but of very limited usefulness in its current form. For any real world application one would have to redo it using some other technology. pymc is just not up to the task.</p>
</div>



  <head>
    
  </head>

  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">nadbor</span></span>

      




<time class='entry-date' datetime='2017-03-23T21:52:41+00:00'><span class='date'><span class='date-month'>Mar</span> <span class='date-day'>23</span><span class='date-suffix'>rd</span>, <span class='date-year'>2017</span></span> <span class='time'>9:52 pm</span></time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://nadbordrozd.github.io/blog/2017/03/23/missing-data-imputation-with-pymc-part-2/" data-via="" data-counturl="http://nadbordrozd.github.io/blog/2017/03/23/missing-data-imputation-with-pymc-part-2/" >Tweet</a>
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/03/05/missing-data-imputation-with-bayesian-networks/" title="Previous Post: Missing data imputation with bayesian networks in pymc">&laquo; Missing data imputation with bayesian networks in pymc</a>
      
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/03/23/missing-data-imputation-with-pymc-part-2/">Missing Data Imputation With Pymc: Part 2</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/03/05/missing-data-imputation-with-bayesian-networks/">Missing Data Imputation With Bayesian Networks in Pymc</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/09/17/text-generation-with-keras-char-rnns/">Text Generation With Keras char-RNNs</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/29/datamatching-part-3-match-scoring/">Datamatching Part 3: Match Scoring</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/07/22/datamatching-part-2-spark-pipeline/">Datamatching Part 2: Spark Pipeline</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/nadbordrozd">@nadbordrozd</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'nadbordrozd',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/nadbordrozd?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - nadbor -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ds-lore';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://nadbordrozd.github.io/blog/2017/03/23/missing-data-imputation-with-pymc-part-2/';
        var disqus_url = 'http://nadbordrozd.github.io/blog/2017/03/23/missing-data-imputation-with-pymc-part-2/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</html>
