
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Looking for the Text Top Model - DS lore</title>
  <meta name="author" content="nadbor">

  
  <meta name="description" content="Looking for the Text Top Model Aug 12th, 2017 4:49 pm TL;DR: I tested a bunch of neural network architectures plus SVM + NB on several text &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://nadbordrozd.github.io/blog/2017/08/12/looking-for-the-text-top-model/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="DS lore" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65624880-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">DS lore</a></h1>
  
    <h2>words about stuff</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="nadbordrozd.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/interviews">Interviews</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Looking for the Text Top Model</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2017-08-12T16:49:56+01:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>12</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>4:49 pm</span></time>
        
      </p>
    
  </header>


<div class="entry-content"><p><em>TL;DR: I tested a bunch of neural network architectures plus SVM + NB on several text classification datasets. Results at the bottom of the post.</em></p>

<p>Last year I wrote a <a href="http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/">post</a> about using word embeddings like word2vec or GloVe for text classification. The embeddings in my benchmarks were used in a very crude way - by averaging word vectors for all words in a document and then plugging the result into a Random Forest. Unfortunately, the resulting classifier turned out to be strictly inferior to a good old SVM except in some special circumstances (very few training examples but lots of unlabeled data).</p>

<p>There are of course better ways of utilising word embeddings than averaging the vectors and last month I finally got around to try them. As far as I can tell from a brief survey of arxiv, most state of the art text classifiers use embeddings as inputs to a neural network. But what kind of neural network works best? <a href="https://arxiv.org/abs/1607.02501v2">LSTM</a>? LSTM? <a href="https://arxiv.org/pdf/1408.5882v2.pdf">CNN</a>? <a href="https://arxiv.org/pdf/1611.06639.pdf">BLSTM with CNN</a>? There are doezens of tutorials on the internet showing how to implement this of that neural classfier and testing it on some dataset. The problem with them is that they usually give metrics without a context. Someone says that their achieved 0.85 accuracy on some dataset. Is that good? Should I be impressed? Is it better than Naive Bayes, SVM? Than other neural architectures? Was it a fluke? Does it work as well on other datasets?</p>

<p>To answer those questions, I implemented several network architectures in Keras and created a benchmark where those algorithms compete with classics like SVM and Naive Bayes. <a href="https://github.com/nadbordrozd/text-top-model">Here it is</a>.</p>

<p>I intend to keep adding new algorithms and dataset to the benchmark as I learn about them. I will update this post when that happens.</p>

<h3 id="models">Models</h3>
<p>All the models in the repository are wrapped in scikit-learn compatible classes with <code>.fit(X, y)</code>, <code>.predict(X)</code>, <code>.get_params(recursive)</code> and with all the layer sizes, dropout rates, n-gram ranges etc. parametrised. The snippets below are simplified for clarity.</p>

<p>Since this was supposed to be a benchmark of classifiers, not of preprocessing methods, all datasets come already tokenised and the classifier is given a list of token ids, not a string.</p>

<h4 id="naive-bayes">Naive Bayes</h4>
<p>Naive Bayes comes in two varieties - Bernoulli and Multinomial. We can also use tf-idf weighting or simple counts and we can include n-grams. Since sklearn’s vectorizer expects a string and will be giving it a list of integer token ids, we will have to override the default preprocessor and tokenizer.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span><span class="p">,</span> <span class="n">MultinomialNB</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
</span><span class="line">
</span><span class="line"><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span>
</span><span class="line">    <span class="n">preprocessor</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span>
</span><span class="line">    <span class="n">tokenizer</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
</span><span class="line">    <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">&#39;vectorizer&#39;</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">),</span> <span class="p">(</span><span class="s">&#39;model&#39;</span><span class="p">,</span> <span class="n">MultinomialNB</span><span class="p">())])</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="svm">SVM</h4>
<p>SVMs are a strong baseline for any text classification task. We can reuse the same vectorizer for this one.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
</span><span class="line">
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s">&#39;vectorizer&#39;</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">),</span> <span class="p">(</span><span class="s">&#39;model&#39;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">())])</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="multi-layer-perceptron">Multi Layer Perceptron</h4>
<p>In other words - a vanilla feed forward neural network. This model doesn’t use word embeddings, the input to the model is a bag of words.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
</span><span class="line">
</span><span class="line"><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">20000</span>
</span><span class="line"><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">3</span>
</span><span class="line">
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,)))</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;relu&#39;</span><span class="p">))</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,)))</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;relu&#39;</span><span class="p">))</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">))</span>
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">&#39;softmax&#39;</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
</span><span class="line">              <span class="n">optimizer</span><span class="o">=</span><span class="s">&#39;adam&#39;</span><span class="p">,</span>
</span><span class="line">              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;accuracy&#39;</span><span class="p">])</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Inputs to this model need to be one-hot encoded, same goes for labels.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">keras</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
</span><span class="line">
</span><span class="line"><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
</span><span class="line"><span class="n">X</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">sequences_to_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">&#39;binary&#39;</span><span class="p">)</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="bidirectional-lstm">(Bidirectional) LSTM</h4>
<p>This is where things start to get interesting. The input to this model is not a bag of words but instead a sequence word ids. First thing to do is construct an embedding layer that will translate this sequence into a matrix of d-dimensional vectors.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span>
</span><span class="line">
</span><span class="line"><span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">100</span>
</span><span class="line"><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">37</span>
</span><span class="line"><span class="c"># we will initialise the embedding layer with random values and set trainable=True</span>
</span><span class="line"><span class="c"># we could also initialise with GloVe and set trainable=False</span>
</span><span class="line"><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">))</span>
</span><span class="line"><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span>
</span><span class="line">    <span class="n">vocab_size</span><span class="p">,</span>
</span><span class="line">    <span class="n">embedding_dim</span><span class="p">,</span>
</span><span class="line">    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span>
</span><span class="line">    <span class="n">input_length</span><span class="o">=</span><span class="n">max_seq_len</span><span class="p">,</span>
</span><span class="line">    <span class="n">trainable</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Now for the model proper:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Bidirectional</span>
</span><span class="line"><span class="n">units</span> <span class="o">=</span> <span class="mi">64</span>
</span><span class="line"><span class="n">sequence_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">&#39;int32&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="n">embedded_sequences</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">)</span>
</span><span class="line"><span class="n">layer1</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="p">,</span>
</span><span class="line">    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span><span class="line">    <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span><span class="line">    <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line"><span class="c"># for bidirectional LSTM do:</span>
</span><span class="line"><span class="c"># layer = Bidirectional(layer)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">layer1</span><span class="p">(</span><span class="n">embedded_sequences</span><span class="p">)</span>
</span><span class="line"><span class="n">layer2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="p">,</span>
</span><span class="line">    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span><span class="line">    <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span><span class="line">    <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c"># last of LSTM layers must have return_sequences=False</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">final_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">class_count</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;softmax&#39;</span><span class="p">)</span>
</span><span class="line"><span class="n">predictions</span> <span class="o">=</span> <span class="n">final_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This and all the other models using embeddings requires that labels are one-hot encoded and word id sequences are padded to fixed length with zeros:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
</span><span class="line">
</span><span class="line"><span class="n">X</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">class_count</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="franois-chollets-cnn">François Chollet’s CNN</h4>
<p>This is the (slightly modified) architecture from <a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">Keras tutorial</a>. It’s specifically designed for texts of length 1000, so I only used it for document classification, not for sentence classification.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">MaxPooling1D</span>
</span><span class="line">
</span><span class="line"><span class="n">units</span> <span class="o">=</span> <span class="mi">35</span>
</span><span class="line"><span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.2</span>
</span><span class="line">
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="mi">35</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">class_count</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="yoon-kims-cnn">Yoon Kim’s CNN</h4>
<p>This is the architecture from the <a href="https://arxiv.org/abs/1408.5882v2.pdf">Yoon Kim’s paper</a>, my implementation is based on <a href="https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras">Alexander Rakhlin’s</a>. This one doesn’t rely on text being exactly 1000 words long and is better suited for sentences.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Conv1D</span><span class="p">,</span> <span class="n">MaxPooling1D</span><span class="p">,</span> <span class="n">Concatenate</span>
</span><span class="line">
</span><span class="line"><span class="n">z</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
</span><span class="line"><span class="n">num_filters</span> <span class="o">=</span> <span class="mi">8</span>
</span><span class="line"><span class="n">filter_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
</span><span class="line"><span class="n">conv_blocks</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line"><span class="k">for</span> <span class="n">sz</span> <span class="ow">in</span> <span class="n">filter_sizes</span><span class="p">:</span>
</span><span class="line">    <span class="n">conv</span> <span class="o">=</span> <span class="n">Conv1D</span><span class="p">(</span>
</span><span class="line">        <span class="n">filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">,</span>
</span><span class="line">        <span class="n">kernel_size</span><span class="o">=</span><span class="n">sz</span><span class="p">,</span>
</span><span class="line">        <span class="n">padding</span><span class="o">=</span><span class="s">&quot;valid&quot;</span><span class="p">,</span>
</span><span class="line">        <span class="n">activation</span><span class="o">=</span><span class="s">&quot;relu&quot;</span><span class="p">,</span>
</span><span class="line">        <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</span><span class="line">    <span class="n">conv</span> <span class="o">=</span> <span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">conv</span><span class="p">)</span>
</span><span class="line">    <span class="n">conv</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">conv</span><span class="p">)</span>
</span><span class="line">    <span class="n">conv_blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">conv</span><span class="p">)</span>
</span><span class="line"><span class="n">z</span> <span class="o">=</span> <span class="n">Concatenate</span><span class="p">()(</span><span class="n">conv_blocks</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">conv_blocks</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">conv_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">
</span><span class="line"><span class="n">z</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</span><span class="line"><span class="n">z</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&quot;relu&quot;</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</span><span class="line"><span class="n">predictions</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">class_count</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&quot;softmax&quot;</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="blstm2dcnn">BLSTM2DCNN</h4>
<p>Authors of the <a href="https://arxiv.org/abs/1611.06639v1">paper</a> claim that combining BLSTM with CNN gives even better results than using either of them alone. Weirdly, unlike previous 2 models, this one uses 2D convolutions. This means that the receptive fields of neurons run not just across neighbouring words in the text but also across neighbouring coordinates in the embedding vector. This is suspicious because there is no relation between consecutive coordinates in e.g. GloVe embedding which they use. If one neuron learns a pattern involving coordinates 5 and 6, there is no reason to think that the same pattern will generalise to coordinates 22 and 23 - which makes convolution pointless. But what do I know.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPool2D</span><span class="p">,</span> <span class="n">Reshape</span>
</span><span class="line">
</span><span class="line"><span class="n">units</span> <span class="o">=</span> <span class="mi">128</span>
</span><span class="line"><span class="n">conv_filters</span> <span class="o">=</span> <span class="mi">32</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">embedded_sequences</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span>
</span><span class="line">    <span class="n">units</span><span class="p">,</span>
</span><span class="line">    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span><span class="line">    <span class="n">recurrent_dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
</span><span class="line">    <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Reshape</span><span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">max_seq_len</span><span class="p">,</span> <span class="n">units</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">conv_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">MaxPool2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">preds</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">class_count</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</span><span class="line"><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">sequence_input</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h4 id="stacking">Stacking</h4>
<p>In addition to all those base models, I implemented <a href="https://github.com/nadbordrozd/text-top-model/blob/master/ttm/stacking_classifier.py">stacking classifier</a> to combine predictions of all those very different models. I used 2 versions of stacking. One where base models return probabilities, and those are combined by a simple logistic regression. The other, where base models return labels, and XGBoost is used to combine those.</p>

<h3 id="datasets">Datasets</h3>
<p>For the document classification benchmark I used all the datasets from <a href="http://www.cs.umb.edu/~smimarog/textmining/datasets/">here</a>. This includes the 20 Newsgroups, Reuters-21578 and WebKB datasets in all their different versions (stemmed, lemmatised, etc.).</p>

<p>For the sentence classification benchmark I used the <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">movie review</a> polarity dataset and the <a href="http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip">Stanford sentiment treebank</a> dataset.</p>

<h3 id="results">Results</h3>
<p>Some models were only included in document classification or only in sentence classification - because they either performed terribly on the other or took too long to train. Hyperparameters of the neural models were (somewhat) tuned on one of the datasets before including them in the benchmark. The ratio of training to test examples was 0.7 : 0.3. This split was done 10 times on every dataset and each model was tested 10 time. The tables below show average accuracies across the 10 splits.</p>

<p>Without further ado:</p>

<h4 id="document-classification-benchmark">Document classification benchmark</h4>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class="text"><span class="line">model             r8-all-terms.txt    r52-all-terms.txt    20ng-all-terms.txt    webkb-stemmed.txt
</span><span class="line">--------------  ------------------  -------------------  --------------------  -------------------
</span><span class="line">MLP 1x360                    0.966                0.935                 0.924                0.930
</span><span class="line">SVM tfidf 2-gr               0.966                0.932                 0.920                0.911
</span><span class="line">SVM tfidf                    0.969                0.941                 0.912                0.906
</span><span class="line">MLP 2x180                    0.961                0.886                 0.914                0.927
</span><span class="line">MLP 3x512                    0.966                0.927                 0.875                0.915
</span><span class="line">CNN glove                    0.964                0.920                 0.840                0.892
</span><span class="line">SVM 2-gr                     0.953                0.910                 0.816                0.879
</span><span class="line">SVM                          0.955                0.917                 0.802                0.868
</span><span class="line">MNB                          0.933                0.848                 0.877                0.841
</span><span class="line">CNN 37d                      0.931                0.854                 0.764                0.879
</span><span class="line">MNB bi                       0.919                0.817                 0.850                0.823
</span><span class="line">MNB tfidf                    0.811                0.687                 0.843                0.779
</span><span class="line">MNB tfidf 2-gr               0.808                0.685                 0.861                0.763
</span><span class="line">BNB                          0.774                0.649                 0.705                0.741
</span><span class="line">BNB tfidf                    0.774                0.649                 0.705                0.741
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><img src="/images/text_top_model/doc_accuracy_stripplot.png" /></p>

<p><a href="https://github.com/nadbordrozd/text-top-model/blob/master/ttm/document_results.csv">Full results csv.</a></p>

<h4 id="sentence-classification-benchmark">Sentence classification benchmark</h4>
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="text"><span class="line">model               subjectivity_10k.txt    polarity.txt
</span><span class="line">----------------  ----------------------  --------------
</span><span class="line">Stacker LogReg                     0.935           0.807
</span><span class="line">Stacker XGB                        0.932           0.793
</span><span class="line">MNB 2-gr                           0.921           0.782
</span><span class="line">MNB tfidf 2-gr                     0.917           0.785
</span><span class="line">MNB tfidf 3-gr                     0.916           0.781
</span><span class="line">MNB tfidf                          0.919           0.777
</span><span class="line">MNB                                0.918           0.772
</span><span class="line">LSTM GloVe                         0.921           0.765
</span><span class="line">BLSTM Glove                        0.917           0.766
</span><span class="line">SVM tfidf 2-gr                     0.911           0.772
</span><span class="line">MLP 1x360                          0.910           0.769
</span><span class="line">MLP 2x180                          0.907           0.766
</span><span class="line">MLP 3x512                          0.907           0.761
</span><span class="line">SVM tfidf                          0.905           0.763
</span><span class="line">BLSTM2DCNN GloVe                   0.894           0.746
</span><span class="line">CNN GloVe                          0.901           0.734
</span><span class="line">SVM                                0.887           0.743
</span><span class="line">LSTM 12D                           0.891           0.734
</span><span class="line">CNN 45D                            0.893           0.682
</span><span class="line">LSTM 24D                           0.869           0.703
</span><span class="line">BLSTM2dCNN 15D                     0.867           0.656
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p><img src="/images/text_top_model/sent_accuracy_stripplot.png" /></p>

<p><a href="https://github.com/nadbordrozd/text-top-model/blob/master/ttm/sentence_results.csv">Full results csv.</a></p>

<h4 id="conclusions">Conclusions</h4>
<p>Well, this was underwhelming.</p>

<p>None of the fancy neural networks with embeddings managed to beat Naive Bayes and SVM, at least not consistently. A simple feed forward neural network with a single layer, did better than any other architecture.</p>

<p>I blame my hyperparameters. Didn’t tune them enough. In particular, the number of epochs to train. It was determined once for each model, but different datasets and different splits probably require different settings.</p>

<p>And yet, the neural models are clearly doing something right because adding them to the ensemble and stacking significantly improves accuracy.</p>

<p>When I find out what exactly is the secret sauce that makes the neural models achieve the state of the art accuracies that papers claim they do, I will update my implementations and this post.</p>
</div>



  <head>
    
  </head>

  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">nadbor</span></span>

      




<time class='entry-date' datetime='2017-08-12T16:49:56+01:00'><span class='date'><span class='date-month'>Aug</span> <span class='date-day'>12</span><span class='date-suffix'>th</span>, <span class='date-year'>2017</span></span> <span class='time'>4:49 pm</span></time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://nadbordrozd.github.io/blog/2017/08/12/looking-for-the-text-top-model/" data-via="" data-counturl="http://nadbordrozd.github.io/blog/2017/08/12/looking-for-the-text-top-model/" >Tweet</a>
  
  
  
    <div class="fb-like" data-send="true" data-width="450" data-show-faces="false"></div>
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2017/07/18/so-you-think-you-can-stats/" title="Previous Post: So you think you can stats">&laquo; So you think you can stats</a>
      
      
        <a class="basic-alignment right" href="/blog/2017/12/05/what-they-dont-tell-you-about-data-science-1/" title="Next Post: What they don't tell you about data science 1: You are a software engineer first">What they don't tell you about data science 1: You are a software engineer first &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2017/12/10/what-they-dont-tell-you-about-data-science-2-data-analyst-roles-are-poison/">What They Don't Tell You About Data Science 2: Data Analyst Roles Are Poison</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/12/05/what-they-dont-tell-you-about-data-science-1/">What They Don't Tell You About Data Science 1: You Are a Software Engineer First</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/08/12/looking-for-the-text-top-model/">Looking for the Text Top Model</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/07/18/so-you-think-you-can-stats/">So You Think You Can Stats</a>
      </li>
    
      <li class="post">
        <a href="/blog/2017/07/07/loafing-around-with-xgboots/">Loafing Around With XGBoots</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating...</li>
  </ul>
  
  <a href="https://github.com/nadbordrozd">@nadbordrozd</a> on GitHub
  
  <script type="text/javascript">
    $(document).ready(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'nadbordrozd',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>



<section class="googleplus">
  <h1>
    <a href="https://plus.google.com/nadbordrozd?rel=author">
      <img src="http://www.google.com/images/icons/ui/gprofile_button-32.png" width="32" height="32">
      Google+
    </a>
  </h1>
</section>



  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2017 - nadbor -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'ds-lore';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://nadbordrozd.github.io/blog/2017/08/12/looking-for-the-text-top-model/';
        var disqus_url = 'http://nadbordrozd.github.io/blog/2017/08/12/looking-for-the-text-top-model/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id; js.async = true;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

</html>
