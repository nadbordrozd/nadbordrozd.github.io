<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Word2vec | DS lore]]></title>
  <link href="http://nadbordrozd.github.io/blog/categories/word2vec/atom.xml" rel="self"/>
  <link href="http://nadbordrozd.github.io/"/>
  <updated>2016-07-12T21:56:14+01:00</updated>
  <id>http://nadbordrozd.github.io/</id>
  <author>
    <name><![CDATA[nadbor]]></name>
    <email><![CDATA[nadbordrozd@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Deepwalking With Companies]]></title>
    <link href="http://nadbordrozd.github.io/blog/2016/06/13/deepwalking-with-companies/"/>
    <updated>2016-06-13T22:19:12+01:00</updated>
    <id>http://nadbordrozd.github.io/blog/2016/06/13/deepwalking-with-companies</id>
    <content type="html"><![CDATA[<p>I have blogged about the wide usefulness of <a href="http://nadbordrozd.github.io/blog/2015/11/29/ds-toolbox-topic-models/">topic models</a> and I have <a href="http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/">benchmarked</a> word-embedding-assisted classification on Reuter’s benchmark. This time I experiment with these ideas using a real world and decent sized dataset - the graph of UK/Irish companies. I have done this during my “10% time” at <a href="http://www.duedil.com">DueDil</a> (it’s like google’s “20% time”, except it <a href="http://uk.businessinsider.com/google-20-percent-time-policy-2015-4?r=US&amp;IR=T">exists</a>).</p>

<h3 id="graph-of-companies">Graph of companies</h3>
<p>There are 4 million active companies in the UK and Ireland. DueDil collects all kinds of information about them - financials, legal structures, contact info, company websites, blogs, news appearences etc. All of it is presented to our users and some of it also serves as input to machine learning tasks - like classifying companies into industries.</p>

<p>One very interesting dataset that remains underutilised (AFAIK by anyone, not just DueDil) is the network of connections of companies and directors.</p>

<p>You can tell a lot about a company just by looking at its directors. That is - if you know anything about these people. At DueDil we don’t know much more than just their identities. This would be rather useless in the context of a single company. But there are millions of companies and people who serve as their directors more often then not do it many times in their careers at different companies. Knowing that the director’s name is Jane Brown may be useless, but knowing that the director previously held similar positions at three different tech startups is highly relevant. And this is just one director out of many and one type of relationship.</p>

<p>More generally, one can think about companies as nodes in a graph. Two companies are connected iff there is a person who has served as a director at both of them (not necessarily at the same time). I will call this the <strong>company graph</strong>. Here’s a part of the graph containing DueDil.</p>

<p><img src="/images/comp_graph.png"></p>

<p>DueDil is connected to Founders For Good Ltd because our CEO Damian Kimmelman is also a director at the other company.</p>

<p>It is intuitive that the position of a company in this graph tells us something about the company. It is however difficult to do anything with this information unless it is somehow encoded into numbers.</p>

<h3 id="company2vec">Company2Vec</h3>
<p>This is where word embeddings come in. As I mentioned previously, it is possible to apply Word2Vec to a graph to get an embedding of graph nodes as real-valued vectors in a procedure called <a href="http://arxiv.org/abs/1403.6652">DeepWalk</a>. The idea is very simple:</p>

<ol>
  <li>Construct a bunch of random walks on the graph</li>
  <li>Feed the random walks into Word2Vec</li>
</ol>

<p>A random walk is just a sequence of nodes, where the next node is always one of the neighbours of the previous node, chosen at random. Think: Duedil -&gt; Founders For Good Ltd -&gt; Omio Limited.</p>

<p>Word2Vec accepts a collection of documents - where every document is a list of tokens (strings). Here company Id’s play the role of tokens and random walks play the role of documents. It all checks out.</p>

<p>To limit the size of the graph for this proof of concept, I have applied this procedure only to the 2.2 million companies that</p>

<ol>
  <li>are active</li>
  <li>have at least one edge (director in common) to another active company</li>
</ol>

<p>I generated 10 random walks starting at every company, the length of each walk was 40. Training Word2Vec with <a href="https://radimrehurek.com/gensim/models/word2vec.html">gensim</a> on this corpus of $10 \times 40 \times 2200000 = 8.8 \times 10^8$ tokens took over 11h. It also took a machine with 40gb of RAM before it stopped crashing even though the random walks were generated on-line.</p>

<p>Finally I got some vectors out of it - one per company. These vectors themselves were the goal of this project (they can serve as features in ML), but I also made some plots to verify that the algorithm is working as advertised.</p>

<h3 id="pretty-pictures-with-t-sne-and-bokeh">Pretty pictures with t-SNE and Bokeh</h3>
<p>The embedding produced by DeepWalk was 100-dimensional in this case, so I had to do some dimensionality reduction before trying to visualize the vectors. <a href="https://github.com/danielfrg/tsne">t-SNE</a> is perfect for this kind of thing. Here’s a sample of 40000 company vectors embedded in 2D with t-SNE. You can move or zoom in the plot or hover over the dots to see the names of the corresponding companies.</p>

<html lang="en">
    <head>
        <meta charset="utf-8" />
        <title>Bokeh Plot</title>

<link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css" type="text/css" />

<script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.js"></script>
<script type="text/javascript">
    Bokeh.set_log_level("info");
</script>
    </head>
    <body>

        <div class="plotdiv" id="7953daf6-6ca5-48d8-be1e-4cd0ab91f1f8"></div>

        <script type="text/javascript" src="http://nadbordrozd.github.io/javascripts/custom/deepwalk_bokeh/40k_black_g.js"></script>
    </body>
</html>

<p>It worked! You can immediately see that there is some structure in the data and t-SNE has picked up on it (and this is only a tiny sample - 2% of all the datapoints). What does this structure mean? After the graph has beed transformed with DeepWalk and then t-SNE, the position of a company in this plot doesn’t have a simple interpretation but it’s clear that groups of highly interconnected companies will correspond to clusters of points in this plot. And it’s easy to verify just by looking at the names of the companies that this is the case.</p>

<p>Take the big blob in the upper left corner - the companies there:</p>

<ul>
  <li>Edwards Macliammoir Dublin Gate Theatre Productions Limited</li>
  <li>Humanist Association of Ireland Limited</li>
  <li>Kildare Street Management Limited</li>
  <li>Shannon Airport Authority Limited</li>
</ul>

<p>We have discovered the cluster of irish companies! And if you zoom in on the long, narrow appendage sticking out of this cluster towards bottom left - you’ll see companies like:</p>

<ul>
  <li>Tempelhof Aircraft Leasing (Ireland) Limited</li>
  <li>Gallic Dragon Aviation Limited</li>
  <li>Aergen Aircraft Ten Limited</li>
</ul>

<p>… and hundreds more. This is not even cherry-picked. I hereby declare the discoverery of the Irish Aviation Peninsula.</p>

<p>Slightly up and to the right of center there is a smaller scottish cluster recognizable through such companies as</p>

<ul>
  <li>Caledonian Sausage Company</li>
  <li>Edinburgh Tattoo Productions Limited</li>
  <li>Dundee Ice Arena</li>
</ul>

<p>There are many other smaller clusters and it’s actually a fun exercise to try to pinpoint exactly what do the companies in a cluster have in common.</p>

<h4 id="now-in-color">Now in color!</h4>
<p>This was fun if somewhat grim looking. Let’s try to add some color to the plot. The original goal of this project was to get graph-derived features for industry classification. Let’s try using different colors to denote different industries (based on SIC codes). If DeepWalk coordinates are predictive of the industry a company is in, we should expect to see same-colored dots (companies in the same industry) clustering together in the plot. Does this actually happen?</p>

<html lang="en">
    <head>
        <meta charset="utf-8" />
        <title>Bokeh Plot</title>

<link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css" type="text/css" />

<script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.js"></script>
<script type="text/javascript">
    Bokeh.set_log_level("info");
</script>
    </head>
    <body>

        <div class="plotdiv" id="66d4a3dd-9a8a-4d0a-892f-4fd8be74f711"></div>
        <script type="text/javascript" src="http://nadbordrozd.github.io/javascripts/custom/deepwalk_bokeh/40k_ind_g.js"></script>
    </body>
</html>

<p>A little bit, yes.</p>

<p>Mostly everything is a big reddish mess (“services” is the most popular category). But there are indeed some clusters. Right of center we can see a medium sized pink blob of insurance companies:</p>

<ul>
  <li>US Risk (UK) Newco Limited</li>
  <li>Zenith Insurance Management UK Limited</li>
  <li>North Star Underwriting Limited</li>
</ul>

<p>Below it and to the left lies another, this one green:</p>

<ul>
  <li>Timeless Films Limited</li>
  <li>Hercules Productions Ltd</li>
  <li>Koninck studios PTE Limited</li>
</ul>

<p>Clearly this is a cluster of film companies (plus other media). If you look more closely you will discover that this is actually the cluster of <em>London based</em> film companies. Nearby there is a smaller green cluster of media companies from the rest of England and another one for Wales. These are less clearly delimited and partly obscured by the red dots of “Services” companies. There are many others, but they are sometimes so tight, they appear as a single dot in the plot.</p>

<p>This is more noisy than I hoped for but it’s definitely working. Would definitely improve accuracy of industry classification if used with other stronger features. Plus you can learn interesting things from it just by looking at the plot. Like the fact that film production companies are closely connected to each other and relatively unconnected to the rest of the world. Or that London is a different country as far as media companies are concerned.</p>

<h4 id="bonus-keyword-based-company-embedding">Bonus: keyword based company embedding</h4>
<p>Having all this t-SNE and Bokeh niceness in place I couldn’t resist applying it to another interesting dataset - keywords. Keywords are a set of industry related tags that DueDil has for millions of companies. They are things like “fishing” or “management consulting” or “b2b”. A company usually has between a few and a few dozen of them.</p>

<p>A byproduct of the pipeline that extracts keywords for companies is a Word2Vec embedding of the keywords. I used this embedding to create an embedding of companies. This was done simply by averaging all the vectors corresponding to a company’s keywords. I ran the resulting vectors through t-SNE and here’s what it looks like:</p>

<html lang="en">
    <head>
        <meta charset="utf-8" />
        <title>Bokeh Plot</title>

<link rel="stylesheet" href="https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css" type="text/css" />

<script type="text/javascript" src="https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.js"></script>
<script type="text/javascript">
    Bokeh.set_log_level("info");
</script>
    </head>
    <body>

        <div class="plotdiv" id="8fd1f452-080d-4d66-af86-a63a3b60f6dd"></div>
        <script type="text/javascript" src="http://nadbordrozd.github.io/javascripts/custom/deepwalk_bokeh/50k_ind_kw.js"></script>
    </body>
</html>

<p>I shouldn’t be surprised that keywords - which were picked to be industry related - predict the industry really well. But I was blown away by the level of detail preserved by t-SNE. There are recognizable islands for everything. There is a golden Farmers Archipelago and a narrow blue Dentist Island south from Home Care Island. There is a separate Asian Island in the Restaurant Archipelago - go see for yourself.</p>

<p>This was fun. Long live Word2Vec and t-SNE!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Text Classification With Word2Vec]]></title>
    <link href="http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"/>
    <updated>2016-05-20T18:18:58+01:00</updated>
    <id>http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec</id>
    <content type="html"><![CDATA[<p>In the <a href="http://nadbordrozd.github.io/blog/2015/11/29/ds-toolbox-topic-models/">previous post</a> I talked about usefulness of topic models for non-NLP tasks, it’s back to NLP-land this time. I decided to investigate if word embeddings can help in a classic NLP problem - text categorization. Full code used to generate numbers and plots in this post can be found <a href="https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/benchmarking.ipynb">here</a>.</p>

<h4 id="motivation">Motivation</h4>
<p>The basic idea is that semantic vectors (such as the ones provided by Word2Vec) should preserve most of the relevant information about a text while having relatively low dimensionality which allows better machine learning treatment than straight one-hot encoding of words. Another advantage of topic models is that they are unsupervised so they can help when labaled data is scarce. Say you only have one thousand manually classified blog posts but a million unlabeled ones. A high quality topic model can be trained on the full set of one million. If you can use topic modeling-derived features in your classification, you will be benefitting from your entire collection of texts, not just the labeled ones.</p>

<h4 id="getting-the-embedding">Getting the embedding</h4>
<p>Ok, word embeddings are awesome, how do we use them? Before we do anything we need to get the vectors. We can download one of the great pre-trained models from <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a>:
<code>bash
wget http://nlp.stanford.edu/data/glove.6B.zip
unzip glove.6B.zip
</code></p>

<p>and use load them up in python:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="err">“</span><span class="n">glove</span><span class="o">.</span><span class="mi">6</span><span class="n">B</span><span class="o">.</span><span class="mi">50</span><span class="n">d</span><span class="o">.</span><span class="n">txt</span><span class="err">”</span><span class="p">,</span> <span class="err">“</span><span class="n">rb</span><span class="err">”</span><span class="p">)</span> <span class="k">as</span> <span class="n">lines</span><span class="p">:</span>
</span><span class='line'>    <span class="n">w2v</span> <span class="o">=</span> <span class="p">{</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]))</span>
</span><span class='line'>           <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>or we can train a Word2Vec model from scratch with gensim:
<code>python
import gensim
# let X be a list of tokenized texts (i.e. list of lists of tokens)
model = gensim.models.Word2Vec(X, size=100)
w2v = dict(zip(model.index2word, model.syn0))
</code></p>

<h4 id="the-python-meat">The (python) meat</h4>
<p>We got ourselves a dictionary mapping word -&gt; 100-dimensional vector. Now we can use it to build features. The simplest way to do that is by averaging word vectors for all words in a text. We will build a sklearn-compatible transformer that is initialised with a word -&gt; vector dictionary.
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">class</span> <span class="nc">MeanEmbeddingVectorizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class='line'>    <span class="k">def</span> <span class="err">&lt;</span><span class="nf">strong</span><span class="o">&gt;</span><span class="n">init</span><span class="o">&lt;/</span><span class="n">strong</span><span class="o">&gt;</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word2vec</span><span class="p">):</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span> <span class="o">=</span> <span class="n">word2vec</span>
</span><span class='line'>        <span class="c"># if a text is empty we should return a vector of zeros</span>
</span><span class='line'>        <span class="c"># with the same dimensionality as all the other vectors</span>
</span><span class='line'>        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">itervalues</span><span class="p">()</span><span class="o">.</span><span class="n">next</span><span class="p">())</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="bp">self</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
</span><span class='line'>        <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2vec</span><span class="p">]</span>
</span><span class='line'>                <span class="ow">or</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class='line'>        <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">X</span>
</span><span class='line'>    <span class="p">])</span> <span class="sb">``</span><span class="err">`</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">Let</span><span class="err">’</span><span class="n">s</span> <span class="n">throw</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">version</span> <span class="n">that</span> <span class="n">uses</span> <span class="n">tf</span><span class="o">-</span><span class="n">idf</span> <span class="n">weighting</span> <span class="n">scheme</span> <span class="k">for</span> <span class="n">good</span> <span class="n">measure</span>
</span></code></pre></td></tr></table></div></figure>python
class TfidfEmbeddingVectorizer(object):
    def <strong>init</strong>(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(word2vec.itervalues().next())</p>

<pre><code>def fit(self, X, y):
    tfidf = TfidfVectorizer(analyzer=lambda x: x)
    tfidf.fit(X)
    # if a word was never seen - it must be at least as infrequent
    # as any of the known words - so the default idf is the max of 
    # known idf's
    max_idf = max(tfidf.idf_)
    self.word2weight = defaultdict(
        lambda: max_idf, 
        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

    return self

def transform(self, X):
    return np.array([
            np.mean([self.word2vec[w] * self.word2weight[w]
                     for w in words if w in self.word2vec] or
                    [np.zeros(self.dim)], axis=0)
            for words in X
        ]) ```
</code></pre>

<p>These vectorizers can now be used <em>almost</em> the same way as <code>CountVectorizer</code> or <code>TfidfVectorizer</code> from <code>sklearn.feature_extraction.text</code>. Almost - because sklearn vectorizers can also do their own tokenization - a feature which we won’t be using anyway because the benchmarks we will be using come already tokenized. In a real application I wouldn’t trust sklearn with tokenization anyway - rather let spaCy do it.</p>

<p>Now we are ready to define the actual models that will take tokenised text, vectorize and learn to classify the vectors with something fancy like Extra Trees. sklearn’s Pipeline is perfect for this:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">etree_w2v</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
</span><span class='line'>    <span class="p">(</span><span class="err">“</span><span class="n">word2vec</span> <span class="n">vectorizer</span><span class="err">”</span><span class="p">,</span> <span class="n">MeanEmbeddingVectorizer</span><span class="p">(</span><span class="n">w2v</span><span class="p">)),</span>
</span><span class='line'>    <span class="p">(</span><span class="err">“</span><span class="n">extra</span> <span class="n">trees</span><span class="err">”</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">))])</span>
</span><span class='line'><span class="n">etree_w2v_tfidf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([</span>
</span><span class='line'>    <span class="p">(</span><span class="err">“</span><span class="n">word2vec</span> <span class="n">vectorizer</span><span class="err">”</span><span class="p">,</span> <span class="n">TfidfEmbeddingVectorizer</span><span class="p">(</span><span class="n">w2v</span><span class="p">)),</span>
</span><span class='line'>    <span class="p">(</span><span class="err">“</span><span class="n">extra</span> <span class="n">trees</span><span class="err">”</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">))])</span>
</span></code></pre></td></tr></table></div></figure></p>

<h4 id="benchmarks">Benchmarks</h4>
<p>I benchmarked the models on everyone’s favorite <a href="http://www.cs.umb.edu/~smimarog/textmining/datasets/">Reuters-21578</a> datasets. Extra Trees-based word-embedding-utilising models competed against text classification classics - Naive Bayes and SVM. Full list of contestants:</p>

<ul>
  <li>mult_nb - Multinomial Naive Bayes</li>
  <li>bern_nb - Bernoulli Naive Bayes</li>
  <li>svc - linear kernel SVM</li>
  <li>glove_small - ExtraTrees with 200 trees and vectorizer based on 50-dimensional gloVe embedding trained on 6B tokens</li>
  <li>glove_big - same as above but using 300-dimensional gloVe embedding trained on 840B tokens</li>
  <li>w2v - same but with using 100-dimensional word2vec embedding trained on the benchmark data itself (using both training and test examples [but not labels!])</li>
</ul>

<p>Each of these came in two varieties - regular and tf-idf weighted.</p>

<p>The results (on 5-fold cv on a the R8 dataset of 7674 texts labeled with 8 categories):
<code>bash
model                score
-----------------  -------
svc_tfidf           0.9656
svc                 0.9562
w2v_tfidf           0.9554
w2v                 0.9516
mult_nb             0.9467
glove_big           0.9279
glove_big_tfidf     0.9273
glove_small         0.9250
glove_small_tfidf   0.9061
mult_nb_tfidf       0.8615
bern_nb             0.7954
bern_nb_tfidf       0.7954
</code></p>

<p>SVM wins, word2vec-based Extra Trees is a close second, Naive Bayes not far behind. Interestingly, embedding trained on this relatively tiny dataset does significantly better than pretrained GloVe - which is otherwise fantastic. Can we do better? Let’s check how do the models compare depending on the number of labeled training examples. Due to its semi-supervised nature w2v should shine when there is little labeled data.</p>

<p><img src="/images/text_class_with_w2v/r8.png"></p>

<p>That indeed seems to be the case. <code>w2v_tfidf</code>’s performance degrades most gracefully of the bunch. <code>SVM</code> takes the biggest hit when examples are few. Lets try the other two benchmarks from Reuters-21578. 52-way classification:</p>

<p><img src="/images/text_class_with_w2v/r52.png"></p>

<p>Qualitatively similar results.</p>

<p>And 20-way classification:</p>

<p><img src="/images/text_class_with_w2v/20ng.png"></p>

<p>This time pretrained embeddings do better than Word2Vec and Naive Bayes does really well, otherwise same as before.</p>

<h4 id="conclusions">Conclusions</h4>
<ol>
  <li>SVM’s are pretty great at text classification tasks</li>
  <li>Models based on simple averaging of word-vectors can be surprisingly good too (given how much information is lost in taking the average)</li>
  <li>but they only seem to have a clear advantage when there is ridiculously little labeled training data</li>
</ol>

<p>At this point I have to note that averaging vectors is only the easiest way of leveraging word embeddings in classification but not the only one. You could also try embedding whole documents directly with <a href="https://radimrehurek.com/gensim/models/doc2vec.html">Doc2Vec</a>. Or use Multinomial Gaussian Naive Bayes on word vectors. I have tried the <a href="https://github.com/nadbordrozd/blog_stuff/blob/master/classification_w2v/multi_multi_kernel_nb.py">latter approach</a> but it was too slow to include in the benchmark.</p>

<ol>
  <li>Sometimes pretrained embeddings give clearly superior results to word2vec trained on the specific benchmark, sometimes it’s the opposite. Not sure what is going on here.</li>
</ol>

<p>Overall, we won’t be throwing away our SVMs any time soon in favor of word2vec but it has it’s place in text classification. Like when you have a tiny training set or to ensemble it with other models to gain edge in Kaggle.</p>

<p>Plus, can SVM do this:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="err">‘</span><span class="n">Berlin</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">London</span><span class="err">’</span><span class="p">],</span>
</span><span class='line'>     <span class="p">[</span><span class="err">‘</span><span class="n">cow</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">cat</span><span class="err">’</span><span class="p">],</span>
</span><span class='line'>     <span class="p">[</span><span class="err">‘</span><span class="n">pink</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">yellow</span><span class="err">’</span><span class="p">]]</span>
</span><span class='line'><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="err">‘</span><span class="n">capitals</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">animals</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">colors</span><span class="err">’</span><span class="p">]</span>
</span><span class='line'><span class="n">etree_glove_big</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">h1</span> <span class="nb">id</span><span class="o">=</span><span class="s">&quot;never-before-seen-words&quot;</span><span class="o">&gt;</span><span class="n">never</span> <span class="n">before</span> <span class="n">seen</span> <span class="n">words</span><span class="err">!!!</span><span class="o">&lt;/</span><span class="n">h1</span><span class="o">&gt;</span>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">test_X</span> <span class="o">=</span> <span class="p">[[</span><span class="err">‘</span><span class="n">dog</span><span class="err">’</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="n">red</span><span class="err">’</span><span class="p">],</span> <span class="p">[</span><span class="err">‘</span><span class="n">Madrid</span><span class="err">’</span><span class="p">]]</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">print</span> <span class="n">etree_glove_big</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
</span><span class='line'><span class="o">&lt;</span><span class="n">code</span><span class="o">&gt;</span>
</span><span class='line'><span class="n">prints</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;</span><span class="n">text</span>
</span><span class='line'><span class="p">[</span><span class="err">‘</span><span class="n">animals</span><span class="err">’</span> <span class="err">‘</span><span class="n">colors</span><span class="err">’</span> <span class="err">‘</span><span class="n">capitals</span><span class="err">’</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DS Toolbox - Topic Models]]></title>
    <link href="http://nadbordrozd.github.io/blog/2015/11/29/ds-toolbox-topic-models/"/>
    <updated>2015-11-29T14:59:33+00:00</updated>
    <id>http://nadbordrozd.github.io/blog/2015/11/29/ds-toolbox-topic-models</id>
    <content type="html"><![CDATA[<p>If you’re not primarily working with NLP you may not have been paying attention to topic modeling and word embeddings. In this post I intend to convince you that you should.</p>

<h3 id="topic-models">Topic models</h3>
<p>Topic models are a set of models in NLP that discover common themes in a collection of documents. You give it a list of texts and it comes up with a bunch of topics and maps every document to a topic or a mixture of topics. <a href="http://www.princeton.edu/~achaney/tmve/wiki100k/browse/topic-presence.html">Here</a> you can see a visualization of a topic model applied to wikipedia articles. As you can see, it picks up similar kinds of themes in the texts that a human being would notice. Take topic 5 for instance. Its top relevant words are “war”, “force”, “army”, “attack”, “military” and top articles: “Second Boer War”, “Erwin Rommel”, “Axis powers”, “Vietnam war”. Pretty neat. Most topic models (like the most popular <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA</a> produce a vector of numbers for every text - the distribution of topics and a similar vector for every word - the affinity of the word to every topic.</p>

<h3 id="word-and-document-embeddings">Word (and document) embeddings</h3>
<p>Word embeddings are related to topic models but instead of mapping a text to a mixture of topics they learn to map words to real valued fixed-size vectors. The mapping is designed to preserve the semantic structure so that the distance between vectors corresponds to distance in meaning of words. An additional property of these embeddings (probably unintended) is that you can do a sort of algebra on words - like this:</p>

<script type="math/tex; mode=display">vector("Paris") - vector("France") + vector("Italy") \approx vector("Rome")</script>

<p>- an examples taken from the most famous word embedding algorithm  -<a href="https://code.google.com/p/word2vec/">word2vec</a>. How cool is that?
 If you want to play with the vectors yourself and discover new fun analogies ($Bush - Iraq + Vietnam \approx Nixon$ !!!) you can download pretrained vectors from word2vec or <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a>.</p>

<h3 id="why-you-should-care">Why you should care</h3>
<p>Unsupervised algorithms learning about analogies between real-world entities is pretty cool but it obscures the wider applicability of these algorithms. The topic modeling and embedding algorithms were invented in the context of NLP but most of them (including LDA and wor2vec) don’t have any inherent knowledge about natural languages and can be applied just as well to sequences of items from a discrete set of any kind. This is huge. Sequences (or sets) of items are notoriously hard to work with. Most popular ML algorithms expect fixed-size vectors as inputs. If your inputs are sequences you can either:</p>

<ul>
  <li>do one-hot encoding and then use any old ML algo you want (like a random forest). Unfortunately one-hot discards all information about the order of items. More importantly, as the size of the vocabulary grows beyond thousands (which is still tiny as far as vocabularies go) your random forest will take forever to train and overfit.</li>
  <li>use naive bayes. NB works surprisingly well but only for classification and only when the problem is - well, naive. It also utilizes the item order information to a very limited extent via n-grams.</li>
  <li>use a <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">recurrent neural network</a> This can actually be very effective in some cases but I don’t think it would work well with a vocabulary size in the thousands. Even the character-level language models take days to train properly on a GPU (but what incredible results it produces!). I believe NNs of all kinds will get better and easier to use but as of now this is not a practical solution to our problem at all.</li>
</ul>

<p>This is where word embeddings come in. Just run word2vec on your sequences of items and you’ll get a reasonably-dimensional representation of every item. You can then take the mean of all vectors in a sequence to get a representation of sequences. Or run <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">doc2vec</a> and you’ll get a vector for each sequence directly. Or if you need them clustered - run LDA. LDA’s word-topic coefficients can also be used as word embedding but they don’t do nearly as good of a job at it as word2vec. Same goes for LDA’s text-topic coefficients as a document to vector mapping.</p>

<p>This is it. This is what topic modeling buys you. It’s a generic clustering and feature extraction technique that works on sequences (or sets) of items from a discrete vocabulary. Does it actually work in practice? I don’t know of a lot of examples of people using it but I know a few.</p>

<h4 id="page-jumps">Page jumps</h4>
<p>As I have already described in my <a href="http://nadbordrozd.github.io/interviews/">mini-guide to data science interviews</a> (question “Predicting page jumps”) it can be used to model users jumping between pages of a web application. Here a page plays the role of a word and a user journey is a sentence. You can (and from what I understood from the interview - they [the company I interviewed with] do) use topic modeling to segment users based on their journeys and extract features for them to predict page jumps.</p>

<h4 id="ad-clicks">Ad clicks</h4>
<p>In a similar vein, topic modeling <a href="http://www.cs.kumamoto-u.ac.jp/~yasuko/PUBLICATIONS/kdd12-trimine.pdf">has been used</a> as a feature extraction technique in the prediction of ad clicks. The paper concentrates on the authors’ special brand of topic modeling but the idea is simple and can be used with any topic model for example LDA. They treat hosts (the websites with banners on them) as words and users who visit the websites as documents. Let’s say John visits youtube then google then wikipedia then youtube again then tmz then guardian. This gives us a sequence [“youtube”, “google”, “wikipedia”, “youtube”, “tmz”, “guardian”]. Topic modeling is applied to the set of all users and the result is a set of topics (“media”, “business” and “drive” in the paper) and a decomposition of every user into those topics. So for our John we should expect something like <code>{media: 0.8, business: 0.2, drive: 0}</code>. This is interesting in itself and constitutes a great feature vector that you can feed into a regression predicting clicks - which is exactly what the authors did.</p>

<h4 id="deepwalk">DeepWalk</h4>
<p>Feature extraction from graphs is even harder than for sequences. Say you want to classify youtube videos. You have a labeled training set and a set of features for every video. But you also know that some videos link to other videos - forming a graph. Information about the position of the video in this graph is bound to be useful for classifiaction (think “oh, I’m in that weird part of youtube again”) but how do you use it. Authors of the <a href="http://arxiv.org/abs/1403.6652">DeepWalk</a> paper compared several approaches and the best turned out to be a trick involving word2vec that they invented. This time the role of a word is played by a node in the graph (a youtube video). What plays the role of a sentence? For that a collection of short random walks on the graph is constructed - starting at every node of the graph. Think about what happens when a youtube video ends and another one starts playing, and then another one and another. Do this a few times for every video on youtube and you have a corpus of texts. Authors of the paper applied word2vec to those sequences to get vector embeddings for videos and then used these vectors as features in classification. It worked better then all other approaches - even ones that use global features of the graph. Awesome.</p>

<h4 id="frequent-itemsets">Frequent itemsets</h4>
<p>Association rule learning is a popular task in the context of retail. If a customer bought butter and bread, what other item are they likely to buy? The usual approach is to count instances of people buying {bread, butter, X} and divide that number by the count of people buying {bread, butter} - this estimates the probability of buying X. Then you can find the X that maximizes the probability and do something with it (suggest it to the user as they are about to checkout perhaps). This is a bit crude, not very robust and it doesn’t provide any insight, just the prediction. What you can do instead is to run (you guessed it) topic modeling with items playing the role of words and baskets playing the role of sentences. Word2vec will give you vector representations of both items and baskets which will allow you to use more sophisticated algorithms for predicting the next item. You will also get a segmentation of all users and all items for free. To understand why this is superior consider this: topic modeling will easily pick up on the cluster of vegetarian buyers and then the model will know not to recommend the buyer pork chops even if they bought three other items that usually go with bacon - this is something frequent itemset algorithms are incapable of. When a new type of soy-based pork substitute appears on the shelves, the algorithm will also take much less time to figure out that it belongs to the vegetarian cluster and is analogous to meat. I don’t actually know if anyone in retail is doing topic modeling on baskets but if they don’t, they should. I’ll do it myself if I can find a free dataset with retail baskets.</p>

<p>If you know of any other cool applications of topic modeling to non-NLP problems let me know.</p>

<p>If you want to play with topic models yourself I wholehartedly recommend <a href="https://radimrehurek.com/gensim/">gensim</a>. I tried also MLLib but its word2vec implementation required 3 times as much RAM (for each of 10 cores I used) and still was about ten times slower than gensim.</p>
]]></content>
  </entry>
  
</feed>
